\chapter{Methodology}
\label{chap:methodology}

\iffalse
The type of research you did
How you collected your data
How you analyzed your data
Any tools or materials you used in the research
Your rationale for choosing these methods

\begin{enumerate}
    \item What is the state of the art for real time event correlation?
    \item How can we improve the way real time event correlation is done for Windows Event Logs? %How can multi-threaded programming language and better rule formats improve how real time event...
    \item What is the performance of our proposed method, and how does it compare to other methods?
\end{enumerate}

\fi

One of the main goals of this thesis is to explore if there is any way that we can improve the way real time event correlation is done and how our improvement compare to other methods. As outlined in \cref{chap:background} we have chosen to compare our solution against \acrfull{sec}. In addition to the focus towards \acrshort{sec}, we will particularly look at event correlation of Windows Event Logs. In the following chapter we will discuss the methodology used address these goals. We will evaluate which datasets exist and should be used, we will discuss the various ways we can improve how event correlation can be done, and we will take a look at how that performance change can be measured.

\section{Datasets}
\label{sec:datasets}
To properly address the research questions proposed, it is important to have one or more datasets that can be used to evaluate the performance of the proposed solution in this thesis.
There is not a vast variety of available datasets that focus on Windows Event Logs publicly available, but there are some that have surfaced in the recent years. We will present those in the following section and offers a short evaluation in the context of this thesis.

\subsection{Evaluation of existing datasets}
\label{sub:evaluation-of-existing-datasets}
When evaluating which datasets we want to use for our experiments, we first have to define some parameters that we can measure the datasets by:
\begin{itemize}
    \item Size - The dataset must be large enough to measure the performance of existing solutions and our proposed solution.
    \item Representative - The dataset must be representative of the real world
    \item Up to date - The dataset should preferably be of a recent date
\end{itemize}

\subsubsection{Boss of the SOC}
Boss of the SOC (BOTS) are datasets created for Splunks Boss of the SOC capture the flag competitions \cite{bossofthesocdatasets}. The datasets are created in a controlled environment, where some adversarial actions has taken place. The contestants have to analyze and hunt in the data to answer several security-related questions which grant points.

There are currently three different datasets available. Each with a different focus. The first dataset consists primarily of Suricata \cite{Suricata49:online} and Windows events. The second dataset also contains Suricata and Windows events, in addition to more application specific logs like Symantec Endpoint Protection, Weblogic, MySQL etc. The third and last dataset focuses more on cloud and hybrid environments and do not contain the same amount of Windows event logs for instances.

The datasets from BOTS are released in a Splunk pre-indexed format, meaning that one would have to set up a Splunk instance, import the indexed datasets, and then export the datasets out in a more usable format (like JavaScript Object Notation (JSON)). 

\subsubsection{Mordor}
The Mordor datasets \cite{mordor_datasets} are pre-recorded events generated by simulating adversarial techniques in a test environment using common red team tools like Empire and Cobalt Strike.

There currently exists two datasets under the Mordor project, namely APT29 and APT3. These datasets contain Windows event logs from simulated Advanced Persistent Threat (APT) actions. These actions are predefined by the MITRE ATT\&CK Evaluations project \cite{MITREevals}. The MITRE ATT\&CK Evaluations project is created as a way to evaluate different endpoint solutions ability to detect various adversarial techniques, tactics and procedures. The adversarial actions are maps to techniques under 10 categories in the MITREs ATT\&CK Matrix \cite{MITREmatrix}, as shown in \cref{tab:mitre-attack-categories}.

\begin{table}[htbp]
\centering
\begin{tabular}{|l|}
\hline
Categories \\ \hline
Initial Access \\
Execution \\
Persistence \\
Privilege Escalation
Defense Evasion \\
Credential Access
Discovery \\
Lateral Movement \\
Collection \\
Command and Control \\
Exfiltration \\
Impact \\ \hline
\end{tabular}
\caption{List of MITRE ATT\&CK Matrix categories}
\label{tab:mitre-attack-categories}
\end{table}

We will focus on the APT3 dataset. This dataset consists of two subsets, one for each scenario as outlined by MITRE in their Attack Emulation Overview \cite{MITREattackevalapt3} for APT3.

\subsubsection{Synthetic datasets}
Synthetic data is datasets that are generated and design with the intent to measure some specific condition or event that may not be found in real world data, or that the real world data would be hard to come by, as told by \textcite{barse2003synthesizing}. There are multiple reasons why one might consider to use a synthetic dataset, like simulating a large period of time which would be unrealistic to capture in real life, simulating extraordinary events occurring, huge data loads, and so forth. Continuing this section, we will consider three different synthetic datasets that we will be applying during our experiments in \cref{chap:experiments}.

It is worth stressing that the synthetic datasets are used strictly for measuring the performance of the systems in a worst-case/best-case scenario, and the dataset is in itself not representative of a real world scenario.

\textbf{Baseline dataset}\\
This dataset is a dataset with events that are all benign. This dataset is useful for measuring the speed at which the tools process and analyse the events, without triggering any rules.

\textbf{High signal, low noise dataset}\\
If we want to test the maximum event correlation throughput possible, we want to use a dataset that is designed to continuously trigger one or more rules. Given that we want to trigger a rule like the one defined in \cref{sigma-example-rule}, a high signal, low noise dataset could be designed to repeat the same 3 log lines that are enough to trigger the rule. 

\textbf{Low signal, high noise dataset}\\
This is the opposite of the high signal, low noise dataset which contained the necessary log lines to repeatedly trigger a specific rule. This dataset only contains the necessary events to trigger a single rule once, the rest of the events are simply background noise. This is pretty similar to the baseline dataset.


\subsection{Datasets used in this thesis}
\label{sec:datasets-used}

For the experiments conducted in this thesis, Multiple datasets have been chosen:

\begin{itemize}
    \item Mordor dataset (APT3, Scenario 1 and 2)
    \item High signal, low noise dataset
    \item Baseline dataset
\end{itemize}
We chose the Mordor dataset as it is sufficiently large enough, representative and up-to-date. Then we have chosen the baseline and high signal, low noise synthetic datasets as they will be used for baselining and giving us best and worst-case scenarios for performance measuring. We do not use the low signal, high noise dataset, as we consider that almost identical to the baseline dataset.

\section{Improving real time event correlation for Windows Event Logs}
\label{sec:improving-real-time-event-correlation-for-windows-event-logs}

With regards to Research Question 2 in Section \cref{sec:researchquestions}, the question we are trying to answer is if there are ways we can improve how real time event correlation is done. We will discuss multiple approaches to how this can be achieved in the following section.

\subsection{Compiled language vs. interpreted language}
\label{sub:use-compiled-language}


As previously stated in \cref{sec:SEC}, \acrshort{sec} is written in the Perl language. Perl is an interpreted language that according to its creator \textcite{wall1994perl} is "optimized for scanning arbitrary text files, extracting information from those text files, and printing reports based on that information". Being an interpreted language means that the code is not compiled into machine code and executed like a compiled language does, but the interpreter parses the code step-by-step and execute its actions in subroutines. We can see an overview of both in \cref{fig:compiled-and-interpreted-language}.


There are many benefits to choosing a interpreted language. The interpreter can hide a lot of the complexity when programming, which means that a interpreted language can be easier to write, use and understand. Similarly to the benefits seen with the rules in rule-based event correlation \cref{sub:rule-based-event-correlation}, the programming language can be written with a close similarity to the human language. Additionally, the programs can run cross-platform, as the interpreter manages the lower level details of the specific architecture that we are executing code on.

The main disadvantage is the additional overhead required by the interpreter. Compiled code will generally always be faster than interpreted code, because it runs closer to the "bare metal". When we want to increase performance, working with compiled languages are generally considered the right thing to do.

In the compiled language world, C and C++ has been the kings for a long time. In recent years, other languages like Go \cite{golang} and Rust \cite{Rust} has seen the light of day, and are increasing in popularity. Benefits of the new generation of compiled languages is the built in features for memory safety, safe concurrency, security and better designed languages that makes it easier to get started with the language. This has been the main issue with traditional compiled programs, they are harder to write and easier to get wrong than a program written in a interpreted language.

While this section might give the impression that there is a black and white difference between compiled and interpreted languages, that is not technically correct. In modern times, languages such as Lisp and Pascal implement both, and Java and C\# are compiled into an intermediate language (bytecode) which is executed in a virtual machine as described in \textcite{henriques2018performance}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{.45\textwidth}
        \centering
        \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto, node distance=1.5cm, thick,
                    exe/.style={rectangle, minimum width=1cm, minimum height=1cm, rounded corners, text centered, draw=black, fill=red!30},
                    stepp/.style={rectangle, minimum width=1cm, minimum height=1cm,text centered, draw=black, fill=orange!30},
                    libraryy/.style={rectangle, rounded corners, minimum width=1cm, text centered, draw=black, fill=green!30}]
        \node (Source) [stepp] {Source};
        \node (Compilation) [libraryy, below of=Source] {Compilation};
        \node (Executable) [exe, below of=Compilation] {Executable};
        \node (Input) [stepp, left of=Executable, xshift=-1cm] {Input};
        \node (Output) [stepp, right of=Executable, xshift=1cm] {Output};
        
        \path[every node/.style={font=\sffamily\small}]
        (Source)
            edge node {} (Compilation)
        (Compilation)
            edge node {} (Executable)
        (Input)
            edge node {} (Executable)
        (Executable)
            edge node {} (Output)
        ;
        \end{tikzpicture}
        \caption{Compiled language model}
        \label{sfig:compiled-language}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{.45\textwidth}
        \centering
        \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto, node distance=1.5cm, thick,
                    exe/.style={rectangle, minimum width=1cm, minimum height=1cm, rounded corners, text centered, draw=black, fill=red!30},
                    stepp/.style={rectangle, minimum width=1cm, minimum height=1cm,text centered, draw=black, fill=orange!30},
                    libraryy/.style={rectangle, rounded corners, minimum width=1cm, text centered, draw=black, fill=green!30}]
        \node (Source) [stepp] {Source};
        \node (Input) [stepp, right of=Source] {Input};
        \path (Source) -- (Input) coordinate[midway] (aux);
        \node (Interpreter) [libraryy, below of=aux] {Interpreter};
        \node (Output) [stepp, below of=Interpreter] {Output};
        
        \path[every node/.style={font=\sffamily\small}]
        (Source)
            edge node {} (Interpreter)
        (Input)
            edge node {} (Interpreter)
        (Interpreter)
            edge node {} (Output)
        ;
        \end{tikzpicture}
        \caption{Interpreted language model}
        \label{sfig:interpreted-language}
    \end{subfigure}
    \caption{Illustration of compiled vs. interpreted language}
    \label{fig:compiled-and-interpreted-language}
\end{figure}


\subsection{Concurrent execution}
\label{sub:concurrent-execution}

As discussed, \acrshort{sec} is not taking full advantage of the system when only running in a single thread. It is a fair claim that by using multi-threading it is possible to increase the throughput of an alternate solution which will process events much quicker. We can symbolize the difference with the synchronous example in \cref{gantt:synchronous-execution}, and the concurrent process as seen in \cref{sub:concurrent-execution}.

While they both process the same amount of events, the concurrent version handles the total number of events much quicker than the synchronous version. Note that it is not given that each individual event is processed any faster in any of the two examples. In fact, given that we probably want to correlate between the events, the concurrent version could use longer time to handle each event, as it has to check a shared context between the threads, which could cause some overhead.

\begin{figure}[ht]
  \centering
\begin{ganttchart}[
    y unit title = 0.6cm, title height=1,
    vgrid={*1{draw=black!15, line width=.75pt}},
    hgrid,
]{1}{16}
  \gantttitlelist{1,...,16}{1} \\
  \ganttbar[bar/.append style={fill=red!50, rounded corners=3pt}]{Thread 1}{1}{2}
  \ganttbar[bar/.append style={fill=blue, rounded corners=3pt}]{}{3}{4}
  \ganttbar[bar/.append style={fill=red!50, rounded corners=3pt}]{}{5}{6}
  \ganttbar[bar/.append style={fill=blue, rounded corners=3pt}]{}{7}{8}
  \ganttbar[bar/.append style={fill=red!50, rounded corners=3pt}]{}{9}{10}
  \ganttbar[bar/.append style={fill=blue, rounded corners=3pt}]{}{11}{12}
  \ganttbar[bar/.append style={fill=red!50, rounded corners=3pt}]{}{13}{14}
  \ganttbar[bar/.append style={fill=blue, rounded corners=3pt}]{}{15}{16}
\end{ganttchart}
  \caption{Synchronously processing of 8 events}
  \label{gantt:synchronous-execution}
\end{figure}

\begin{figure}[ht]  %t top, b bottom, p page | you can also use h to try to get the figure to appear at the current location
  \centering
\begin{ganttchart}[
    y unit title = 0.6cm, title height=1,
    vgrid={*1{draw=black!15, line width=.75pt}},
    hgrid
]{1}{16}
  \gantttitlelist{1,...,16}{1} \\
  \ganttbar[bar/.append style={fill=red!50, rounded corners=3pt}]{Thread 1}{1}{2}
  \ganttbar[bar/.append style={fill=blue, rounded corners=3pt}]{}{3}{4} \\
  \ganttbar[bar/.append style={fill=blue, rounded corners=3pt}]{Thread 2}{1}{2}
  \ganttbar[bar/.append style={fill=red!50, rounded corners=3pt}]{}{3}{4}\\
  \ganttbar[bar/.append style={fill=red!50, rounded corners=3pt}]{Thread 3}{1}{2}
  \ganttbar[bar/.append style={fill=blue, rounded corners=3pt}]{}{3}{4}\\
  \ganttbar[bar/.append style={fill=blue, rounded corners=3pt}]{Thread 4}{1}{2}
  \ganttbar[bar/.append style={fill=red!50, rounded corners=3pt}]{}{3}{4}
\end{ganttchart}
  \caption{Concurrent processing of 8 events}
  \label{gantt:concurrent-execution}
\end{figure}


\subsection{Better rules}
\label{sub:better-rules}

The rules in the knowledge base is the bread and butter of the rule-based event correlation. And although there are multiple different ways to create rules as discussed in \cref{sec:rules}, it is always worth considering if other rule formats could be more beneficial.
As stated by \textcite{rouillard2004real}, the majority of the computational time used in \acrshort{sec} is spent on matching events against regular expression, so if we could in some way remove the need for the extensive use of regular expressions by using another rule format, that could potentially be a much faster solution.

We outlined a possible rule candidate in \cref{sub:sigma}, namely Sigma. We will look at implementing Sigma when we experiment with the rule change.

\subsection{Proper time management}
\label{sub:time-management}

One of the biggest drawbacks of \acrshort{sec} as outlined in \cref{sec:SEC}, is the fact that when it bases its correlation time on when the event was read from the input file. It does not take into account any timestamps that may be in the logs.
If logs are ingested from multiple systems (like in a enterprise environment) the logs could be delayed for multiple reasons, or if \acrshort{sec} is unable to ingest the log events fast enough (either because of I/O delays or a huge amount of logs), the timestamp of the logs will be different from when the log event was \textit{actually} produced. The consequences of this could be severe, as events that \textit{should} be correlated together in a given timeframe might drift away from each-other and not be correlated at all.

Instead of the time being based on when the event is read, we want to base our correlation on when the event was generated by the source system. Since we are doing the assumption that we will only be working with Windows event logs which have the UTC timestamp in the logs, we are able to use that. However, if we were to expand to ingest other logs as well, we would have to take into account that the time might be represented differently in the log. It is rare to see logs that do not have a timestamp in some form or fashion. The hardest part might be localization, if the timestamp is not written with a specific timezone. However, this will not be a problem in this thesis as all Windows event logs are written with the UTC timezone.

\subsection{Internal representation of logs}
\label{sub:internal-rep-of-logs}
When we are testing the different rules in \acrshort{sec} against a log line, the pattern of the rule is applied against the whole log line. We propose that tokenizing the log before testing each rule could improve the performance.

Tokenizing the log means that we are taking a log in the form of "EventID: 1\\nMachineName: client1\\nUser: john", and parsing it into a object instead, as seen in \cref{example-tokenization}. The benefit of this is that we can query specific parts of the event log directly, instead of having to parse the whole event log every time we want to access a single key-value pair. An example could be if we wanted to access the MachineName or User values from \cref{example-tokenization}, which could do something like this  \lstinline{event['MachineName']} and \lstinline{event['User']}.

\begin{lstlisting}[
    caption={Example tokenization},
    label=example-tokenization,
    language=java]
Object event = {
    EventID = 1
    MachineName = client1
    User = john
}
\end{lstlisting}

Moving away from the large regexes as already discussed in \cref{sub:better-rules} and using tokenization to enable using new rule formats like Sigma could improve the performance of our solution.

\subsection{Support for multiple log formats}
\label{sub:multiple-log-formats}

As briefly discussed in \cref{sub:time-management} the biggest hurdle would be event logs that either do not contain a timestamp, or are syntactically hard to parse or tokenize as discussed in \cref{sub:internal-rep-of-logs}.
In this thesis we are focused on Windows Event Logs, but it is possible that other log sources would be possible to have working without any or little change to the solution this thesis proposes. We consider this future work.

\subsection{Output modularity}
\label{sub:modularity}

Defining different alert output channels. It would be nice to be able to create granular output rules that takes some decision based on the alert severity and sends the alert to the proper channel. Channels could include:

\begin{itemize}
    \item E-Mail
    \item Instant Messaging platforms like Slack, Teams et cetera.
    \item Ticketing system
    \item SIEM products like Splunk
\end{itemize}
We have chosen not to implement these as we focused primarily on performance measurements, and consider this future work.

\subsection{Distributed correlation}
\label{sub:distributed-correlation}

There are multiple reasons why we might consider using a distributed correlation system. A distributed system first of all provides redundancy if one or more ingestion node or correlation server should fail, having the system continue running without experiencing loss in data. This is important because when we are correlating, we never know when a rule might hit, and any loss of data or interruptions in the correlation process can lead to missed alerts. Furthermore, with regard to geolocation, being able to reduce latency by ingesting data from hosts as close to them as possible could improve the real time effectiveness of the system. Any system should be able to handle delayed data, but having as little delay when ingesting data is still preferable. Lastly, if we want to scale up our system to handle bigger loads of data and correlation rules, we need scalability.

As discussed in \cref{sec:SEC}, the authors of \textcite{sec-distributed} considered a few ways to scale \acrshort{sec} as shown in \cref{fig:distributed-sec-concept} and \cref{fig:horizontal-scaling-of-sec}.

When scaling a system, we generally consider two different types of scaling. Horizontal and vertical. Horizontal scaling means that we are adding additional machines into a pool of resources for that particular service. Vertical scaling is adding more power to the existing machine, for instance by increasing the available RAM or upgrading the CPU(s).
There are multiple considerations that have to be done when choosing which way to scale a system. Horizontal scaling usually comes with the drawback of having to manage the pool of resources for each scaled service. Vertical scaling is in general simpler, but at some point it is no longer possible or feasible to scale further with regards to performance and cost.
The implementation show in this thesis is primarily built to scale vertically.
Interesting future work would be to add horizontal scaling to the proposed solution in this thesis, much like in \cref{fig:distributed-sec-concept}, and tackle the challenges associated with load balancing, shared "context memory" between the correlators, and other possible obstacles.

\section{Measuring performance}
\label{sec:measuring-performance}
There are multiple factors that affect the performance of event correlators. All these factors lead to multiple ways that we can measure performance. This section tries to outline the most important ones.

\subsection{Data ingestion speed}
\label{sub:ingestion-speed}

At the start of the data pipeline, we have to ingest our data for processing. Data ingestion is the process of importing data from an external source into our program. The rate at which we are ingesting events are usually measured in events per second.
Data ingestion is based on a emitter sending the data, and a receiver receiving the data. The emitter does not have to be a separate system, it can be a hard disk, RAM or a network-based service. 
The performance related to data ingestion speeds can be bottlenecked by several possible things. The emitter may be bound by the storage medium it is sending data from. If we are reading events from a log file stored to disk, we are bound to the read speed that our disk(s) support. If we are reading events from a process that stores the events in memory, we are bound by the read speeds of our RAM.
With regards to network-based transmission, the choice of transport-layer protocol used can have an impact on transmission speed. Using UDP may be the fastest, but could lead to dropped packages which not optimal. Using TCP ensures that all events are transmitted, but at the cost of additional overhead for re-transmitting lost data, re-arranging packets out of order, et cetera. When transmitting data in a network (both internal and via the internet) encryption is needed to ensure authenticity and tamper protection of the data. But encryption comes at a cost, namely that it takes some time to encrypt and decrypt data when its sent and received.
Additionally, the networking hardware can play a role depending on the setup. The supported speeds of the network card in the emitter and receiver, and any intermediate networking equipment like switches and routers could affect the throughput of events.
There are multiple ways of transmitting data over the network that may affect the ingestion speed, and that is the implementation of how transmitting shall be done. Real-time transmission sends the events as soon as they happen, one-by-one. Another tactic is to use batching or chunking that sends bursts of events instead of sending the events one-by-one. Finally, a hybrid approach is possible where the emitter chooses which type to use depending how many events are being transmitted.
Then we have the ingestion capabilities of the receiver. This boils down to how efficient the receiver is to manage the backlog of events it receives. A simple program might only allow processing one event at the time, blocking incoming new events. A more efficient program might store a backlog of events in RAM, which ensures that it does not block incoming new events.

There are multiple ways we can measure all these different possible bottlenecks. For disk and RAM-based operations we can use profiling tools that come with the operating system to measure the load we are under. We can look at the number and size of the network packages being sent and received. Given an external emitter running for instance the software Kafka, we can get an overview of how fast receivers are fetching data from the emitter. Likewise, we can do the same from the end of the receiver by looking at how many events we are ingesting into our program per second. Finally, we can test the ingestion using timing, by ingesting a set number of events and timing how fast the receiver can ingest them (without any processing other than ingesting), we can calculate the number of events per second.

\subsection{Processing speed}
\label{subs:processing-speed}
Since the ingestion speed discussed in \cref{sub:ingestion-speed} might fluctuate depending on how the data is ingested, measuring the internal processing speed might be more interesting when evaluating the performance of the various solutions. This removes the uncertainties related to ingestion speeds. There are multiple options when looking at internal processing speed. One might look at the processing as a whole from start to finish, or try to separate out the various internal steps that occur during processing. Go features a profiler that will output a graph, showing which functions are taking up the most time during runtime. This can give an idea of where the most of the time is spent during processing.

The processing speed can be affected by several things. First of all the dataset used will matter, as the number of matches will have an effect on the number of alerts generated and contexts updated.
Secondly, the implementation of how rules are processed and checked against events can have a big impact on the processing speed. If the solution is able to quickly disregard events as not interesting, there is a big potential for saving time.
Lastly, the internal handling of contextual information, how that information is accessed and other performance-related improvements all have an effect on the processing speed observed.
% Drawback
The biggest drawback of this approach is the need profiling or timing "inside" the solution. While this might be simple to implement in a new solution, patching such a feature into older solutions can prove to be hard or in the worst case error-prone if the solution being patched is not fully understood.

\subsection{Compound processing speed}
\label{sub:compound-processing-speed}

Measuring the compound processing speed will give us a bird's-eye view of what the total processing speed is. It takes into account both ingestion and processing speeds, measures the total time used, including both I/O and any internal processing.

Depending on the solutions, this might be the best or only option for a good one-to-one comparison between solutions, if they do not support the same ingestion abilities.


\section{Test plan}
As discussed in this chapter we have identified  multiple ways that possibly could improve or further expand the capabilities of existing real time event correlation solutions, more specifically \acrshort{sec}.

In \cref{chap:experiments} we will be using the Mordor \cite{mordor_datasets} APT3 dataset, in addition to three synthetic datasets as explained in \cref{sec:datasets}.

We will focus our experiments around the possible improvements mentioned in this chapter, namely using a compiled language, utilize concurrent execution, test if better internal representation of log data and using other rules might affect performance and lastly implement better time management. Distributed correlation, output modularity and support for multiple log formats is considered future work.
As discussed in \cref{sec:measuring-performance}, there are multiple ways that we can measure the performance of our solution against existing solutions. We will be using compound processing speed as discussed in \cref{sub:compound-processing-speed} for our performance tests.